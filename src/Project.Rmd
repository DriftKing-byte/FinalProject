---
title: "Project"
output:
  flexdashboard::flex_dashboard:
    vertical_layout: scroll
---


EDA
============

Exploratory Data Analysis

What is the effect of various factors of a song on its popularity and how do ensemble methods vs. neural network perform on predicting a songâ€™s popularity? 
-------

```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
data <- read.csv("../data/Top_Songs_On_Spotify.csv")
```

### Song Popularity Dataframe
```{r, results='hide'}
str(data)
head(data)
```

```{r}
library(DT)

# Make cor_matrix scrollable
datatable(head(data, 10), 
          options = list(
            scrollX = TRUE,   # horizontal scroll
            scrollY = "400px", # vertical scroll
            paging = FALSE    # show all rows in scroll
          ))

```


```{r}
numeric_df <- data[, sapply(data, is.numeric)]
cor_matrix <- cor(numeric_df)
```



### Correlation Matrix for Features
```{r}
library(DT)

# Make cor_matrix scrollable
datatable(cor_matrix, 
          options = list(
            scrollX = TRUE,   # horizontal scroll
            scrollY = "400px", # vertical scroll
            paging = FALSE    # show all rows in scroll
          ))

```


```{r}
data <- data[, -c(1, 2, 4, 7, 9, 11)]
```

```{r, results='hide'}
install.packages("fastDummies")
library(fastDummies)
```
```{r}
df <- dummy_cols(
  data,
  remove_selected_columns = TRUE,     
  remove_first_dummy = FALSE          
)
```


```{r, results='hide'}
head(df)
```

EDA Continued
==============
Deleting negative correlations columns shown in correlation matrix and columns 1 and 2 (X and title) resulted in lowest RMSE

One-hot encoded columns. This converted the categorical columns to numerical. For example, the artist column.

```{r}
library(DT)

# Make cor_matrix scrollable
datatable(head(df[,c(1:12)]), 
          options = list(
            scrollX = TRUE,   # horizontal scroll
            scrollY = "400px", # vertical scroll
            paging = FALSE    # show all rows in scroll
          ))

```



```{r, results='hide'}
dim(df)
```


Model Comparison: Random Forest Vs. Neural Network
=========



-------------
```{r}
set.seed(123)

train_data <- sample(1:nrow(df), 482)

train <- df[train_data,]
test <- df[-train_data,]
```

```{r, results='hide'}
nrow(train) 
nrow(test)
nrow(train) + nrow(test)
```

```{r}
y_train <- train$pop
x_train <- train[, !names(train) %in% "pop"]

y_test <- test$pop
x_test <- test[, !names(test) %in% "pop"]
```

```{r, results='hide'}
install.packages("randomForest")
```

```{r}
complete_idx <- complete.cases(x_train)

x_train <- x_train[complete_idx, ]
y_train <- y_train[complete_idx]
```


```{r, results='hide'}
library(randomForest)

model1 <- randomForest(
  x = x_train,
  y = y_train,
  ntree = 500,
  mtry = floor(sqrt(ncol(x_train))),
  importance = TRUE
)
```

```{r}
complete_idx_test <- complete.cases(x_test)

x_test_clean <- x_test[complete_idx_test, ]
y_test_clean <- y_test[complete_idx_test]
```

```{r}
preds <- predict(model1, x_test_clean)
```

```{r}
rmse <- sqrt(mean((preds - y_test)^2))
```

Random Forest RMSE
```{r}
rmse
```


```{r}
library(torch)
library(luz)

x_train_tensor <- torch_tensor(as.matrix(x_train), dtype = torch_float())
y_train_tensor <- torch_tensor(as.matrix(y_train), dtype = torch_float())  
x_test_tensor <- torch_tensor(as.matrix(x_test), dtype = torch_float())
y_test_tensor <- torch_tensor(as.matrix(y_test), dtype = torch_float())  
```

```{r}
train_ds <- tensor_dataset(x_train_tensor, y_train_tensor)
test_ds <- tensor_dataset(x_test_tensor, y_test_tensor)
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 32, shuffle = TRUE)
```

```{r}
net <- nn_module(
  "onelayer",
  initialize = function() {
    self$net <- nn_sequential(
      nn_linear(ncol(x_train), 128),
      nn_relu(),
      nn_linear(128, 1)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```


```{r}
model2 <- net %>%
  setup(
    loss = nn_mse_loss(),   # regression loss
    optimizer = optim_adam, 
    metrics = list(luz_metric_rmse())  # mean absolute error
  )
```

-------
Neural Network (Adam Optimizer, 20 epochs)
```{r, results='hide'}
acc <- accelerator(cpu=TRUE)
fitted1 <- model2 %>% 
  fit(
    train_dl, 
    epochs = 20,
    verbose = TRUE,
    accelerator = acc
  )
```


Overfitting not observed in neural network because rmse for test set was lower than train set.


```{r}
evaluate(fitted1, test_dl)
```

Neural Network Plot and Results
================================
```{r}
plot(fitted1)
```

```{r, results='hide'}
install.packages("flexdashboard")
install.packages("crosstalk")
```

### Results

The dnce or "dance" and dB or "decibels" of a song contributed most to the song's popularity.
For model comparison, the ensemble method (Random Forest) achieved a lower RMSE than the neural network.
The Random Forest achieved an RMSE of 12.8689 whereas the neural network achieved an RMSE of 14.207.
This was due to the Random Forest better at capturing the non-linear relationships.
