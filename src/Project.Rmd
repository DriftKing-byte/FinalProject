---
title: "Project"
output:
  flexdashboard::flex_dashboard:
    vertical_layout: scroll
---


EDA
============

Exploratory Data Analysis

What is the effect of various factors of a song on its popularity and how do ensemble methods vs. neural network perform on predicting a songâ€™s popularity? 
-------

```{r}
options(repos = c(CRAN = "https://cloud.r-project.org"))
data <- read.csv("../data/Top_Songs_On_Spotify.csv")
```

### Song Popularity Dataframe
```{r, results='hide'}
str(data)
head(data)
```

```{r, results = 'hide'}
dim(data)
```


```{r}
library(DT)


datatable(head(data, 10), 
          options = list(
            scrollX = TRUE,  
            scrollY = "400px",
            paging = FALSE  
          ))

```


```{r}
numeric_df <- data[, sapply(data, is.numeric)]
cor_matrix <- cor(numeric_df)
```



### Correlation Matrix for Features
```{r}
library(DT)

datatable(cor_matrix, 
          options = list(
            scrollX = TRUE, 
            scrollY = "400px",
            paging = FALSE    
          ))

```


```{r}
data <- data[, -c(1, 7, 10, 12)]
```

```{r, results='hide'}
install.packages("fastDummies")
library(fastDummies)
```
```{r}
df <- dummy_cols(
  data,
  remove_selected_columns = TRUE,     
  remove_first_dummy = FALSE          
)
```


```{r, results='hide'}
head(df)
```

EDA Continued
==============
Deleting negative correlations columns shown in correlation matrix and columns 1 and 2 (X and title) resulted in lowest RMSE

One-hot encoded columns. This converted the categorical columns to numerical. For example, the artist column.

```{r}
head(df[,c(1:12)])
```

Top 10 List of Features for Song Popularity
```{r}
cor_matrix2 <- cor(df)
pop_corr <- cor_matrix2[, "pop"]
pop_corr <- pop_corr[names(pop_corr) != "pop"]
pop_corr_sorted <- sort(abs(pop_corr), decreasing = TRUE)
top_10 <- pop_corr_sorted[1:10]
top_10
```

Best Features
====================

### Best Feature 1: Year
```{r}
library(ggplot2)
ggplot(data, aes(x = year, y = pop)) + 
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm") + 
  labs(
    title = "Song Year vs Popularity",
    x = "Year",
    y = "Popularity"
  ) 
```


### Best Feature 10: dB
```{r}
library(ggplot2)
ggplot(data, aes(x = dB, y = pop)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(method = "lm") + 
  labs(
    title = "Song dB vs Popularity",
    x = "dB",
    y = "Popularity"
  ) 
```


```{r, results='hide'}
dim(df)
```


Model Comparison: Random Forest Vs. Neural Network
=========



-------------
```{r}
set.seed(123)

train_data <- sample(1:nrow(df), 482)

train <- df[train_data,]
test <- df[-train_data,]
```

```{r, results='hide'}
nrow(train) 
nrow(test)
nrow(train) + nrow(test)
```

```{r}
y_train <- train$pop
x_train <- train[, !names(train) %in% "pop"]

y_test <- test$pop
x_test <- test[, !names(test) %in% "pop"]
```

```{r, results='hide'}
install.packages("randomForest")
```

```{r}
complete_idx <- complete.cases(x_train)

x_train <- x_train[complete_idx, ]
y_train <- y_train[complete_idx]
```


```{r, results='hide'}
library(randomForest)

model1 <- randomForest(
  x = x_train,
  y = y_train,
  ntree = 500,
  mtry = floor(sqrt(ncol(x_train))),
  importance = TRUE
)
```

```{r}
complete_idx_test <- complete.cases(x_test)

x_test_clean <- x_test[complete_idx_test, ]
y_test_clean <- y_test[complete_idx_test]
```

```{r}
preds <- predict(model1, x_test_clean)
```

```{r}
rmse <- sqrt(mean((preds - y_test)^2))
```

Random Forest RMSE
```{r}
rmse
```


```{r}
library(torch)
library(luz)

x_train_tensor <- torch_tensor(as.matrix(x_train), dtype = torch_float())
y_train_tensor <- torch_tensor(as.matrix(y_train), dtype = torch_float())  
x_test_tensor <- torch_tensor(as.matrix(x_test), dtype = torch_float())
y_test_tensor <- torch_tensor(as.matrix(y_test), dtype = torch_float())  
```

```{r}
train_ds <- tensor_dataset(x_train_tensor, y_train_tensor)
test_ds <- tensor_dataset(x_test_tensor, y_test_tensor)
train_dl <- dataloader(train_ds, batch_size = 32, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 32, shuffle = TRUE)
```

```{r}
net <- nn_module(
  "onelayer",
  initialize = function() {
    self$net <- nn_sequential(
      nn_linear(ncol(x_train), 128),
      nn_relu(),
      nn_linear(128, 1)
    )
  },
  forward = function(x) {
    self$net(x)
  }
)
```


```{r}
model2 <- net %>%
  setup(
    loss = nn_mse_loss(),   # regression loss
    optimizer = optim_adam, 
    metrics = list(luz_metric_rmse())  # mean absolute error
  )
```

-------
Neural Network (Adam Optimizer, 20 epochs)
```{r, results='hide'}
acc <- accelerator(cpu=TRUE)
fitted1 <- model2 %>% 
  fit(
    train_dl, 
    epochs = 20,
    verbose = TRUE,
    accelerator = acc
  )
```


Overfitting not observed in neural network because rmse for test set was lower than train set.


```{r}
evaluate(fitted1, test_dl)
```

Neural Network Plot and Results
================================
```{r}
plot(fitted1)
```

```{r, results='hide'}
install.packages("flexdashboard")
install.packages("crosstalk")
```

### Results

Of the 10 features that correlated most with a song popularity, the year, artist, title, and dB of a song contributed most to a song's popularity. For model comparison, the ensemble method (Random Forest) achieved a lower RMSE than the neural network. The Random Forest achieved a lower RMSE than the neural network by roughly 1 point. This was due to the Random Forest better at capturing the non-linear relationships.
